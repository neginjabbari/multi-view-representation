{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DiNMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amjad\\AppData\\Local\\Temp\\ipykernel_10488\\4191043127.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X.append(torch.tensor(X0[v]).type(torch.float32))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6289322710724004,\n",
       " 0.659,\n",
       " 0.5035182647389292,\n",
       " 0.659,\n",
       " 0.6549953287547585,\n",
       " 0.6549953287547586,\n",
       " 0.659)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from util import eval, loadDataset, normalization\n",
    "\n",
    "eps = torch.tensor(10 ** -10)\n",
    "\n",
    "# data = torch.load('../Datasets/3Sources.npy')\n",
    "# V = len(data)-1\n",
    "# Y = data[-1].flatten()\n",
    "# c = len(np.unique(Y))\n",
    "\n",
    "# X0 = data[:-1]\n",
    "\n",
    "datasets = ['3Sources.npy',\n",
    " 'BBCSport.npy',\n",
    " 'Caltech101.npy',\n",
    " 'Caltech_2.npy',\n",
    " 'Citeseer.npy',\n",
    " 'Coil100.npy',\n",
    " 'Cora.npy',\n",
    " 'EYaleB10.npy',\n",
    " 'Handwritten.npy',\n",
    " 'MNIST10.npy',\n",
    " 'UCIdigit.npy',\n",
    " 'Umist.npy',\n",
    " 'Yale32.npy',\n",
    " 'Yeast.npy']\n",
    "\n",
    "\n",
    "X0, Y, V, c = loadDataset(8)\n",
    "\n",
    "r = 20\n",
    "alpha = 1\n",
    "beta = 1\n",
    "iter = 300\n",
    "\n",
    "W = []\n",
    "H = []\n",
    "X = []\n",
    "\n",
    "for v in range(V):\n",
    "    \n",
    "    X.append(torch.tensor(X0[v]).type(torch.float32))\n",
    "\n",
    "    # Normalization\n",
    "    # X[v] = normalization(X[v], 'L2col')\n",
    "    \n",
    "    d, n = X[v].shape\n",
    "\n",
    "    W.append(torch.rand(d, r))\n",
    "    H.append(torch.rand(r, n))\n",
    "    \n",
    "\n",
    "    \n",
    "# Optimization\n",
    "err = torch.zeros(iter)\n",
    "\n",
    "for t in range(iter):\n",
    "\n",
    "    for v in range(V):\n",
    "\n",
    "        \n",
    "        \n",
    "        # Updating Ws\n",
    "        Wn = X[v]        @ H[v].T \n",
    "        Wd = W[v] @ H[v] @ H[v].T \n",
    "        W[v] = W[v] * (Wn / torch.maximum(Wd, eps))\n",
    "\n",
    "\n",
    "        # Updating Hs\n",
    "        Q = torch.sum(torch.stack(H), dim=0)\n",
    "        \n",
    "        Hn = 2 * (W[v].T @ X[v])                    \n",
    "        Hd = 2 * (W[v].T @ W[v] @ H[v]) + alpha * (Q - H[v]) + 2 * beta * H[v] \n",
    "        H[v] = H[v] * (Hn / torch.maximum(Hd, eps))\n",
    "\n",
    "        # Calculating cost function\n",
    "        # err[t] += torch.norm(X[v] - W[v] @ H[v]) ** 2 + alpha * (torch.trace(H[v] @ (Q - H[v]).T)) + beta * torch.norm(H[v])**2\n",
    "           \n",
    "# plt.plot(err)\n",
    "\n",
    "Hfinal = torch.mean(torch.stack(H), dim=0)\n",
    "\n",
    "pred = KMeans(n_clusters=c, n_init='auto').fit(Hfinal.T).labels_\n",
    "\n",
    "nmi, acc, ari, f1mi, f1ma, f1we, pur = eval(Y, pred)\n",
    "nmi, acc, ari, f1mi, f1ma, f1we, pur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LP-DiNMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amjad\\AppData\\Local\\Temp\\ipykernel_10488\\1829871985.py:41: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X.append(torch.tensor(X0[v]).type(torch.float32))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7531856071383977,\n",
       " 0.782,\n",
       " 0.6578366645037367,\n",
       " 0.782,\n",
       " 0.7477166686377335,\n",
       " 0.7477166686377335,\n",
       " 0.782)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "\n",
    "from util import eval, loadDataset, normalization\n",
    "\n",
    "eps = torch.tensor(10 ** -10)\n",
    "\n",
    "datasets = ['3Sources.npy',\n",
    " 'BBCSport.npy',\n",
    " 'Caltech101.npy',\n",
    " 'Caltech_2.npy',\n",
    " 'Citeseer.npy',\n",
    " 'Coil100.npy',\n",
    " 'Cora.npy',\n",
    " 'EYaleB10.npy',\n",
    " 'Handwritten.npy',\n",
    " 'MNIST10.npy',\n",
    " 'UCIdigit.npy',\n",
    " 'Umist.npy',\n",
    " 'Yale32.npy',\n",
    " 'Yeast.npy']\n",
    "\n",
    "\n",
    "X0, Y, V, c = loadDataset(8)\n",
    "\n",
    "r = 100\n",
    "k = 6\n",
    "alpha = .1\n",
    "beta = .1\n",
    "gamma = 0.001\n",
    "iter = 500\n",
    "\n",
    "W = []\n",
    "H = []\n",
    "X = []\n",
    "\n",
    "for v in range(V):\n",
    "    \n",
    "    X.append(torch.tensor(X0[v]).type(torch.float32))\n",
    "\n",
    "    # Normalization\n",
    "    # X[v] = normalization(X[v], 'L1row')\n",
    "    \n",
    "    d, n = X[v].shape\n",
    "    \n",
    "    W.append(torch.rand(d, r))\n",
    "    H.append(torch.rand(r, n))\n",
    "    \n",
    "A = []\n",
    "D = []\n",
    "for v in range(V):\n",
    "    \n",
    "    A0 = torch.tensor(kneighbors_graph(X[v].T, k, mode='connectivity', include_self=False).toarray()).type(torch.float32)\n",
    "    A.append(torch.maximum(A0, A0.T))\n",
    "    D.append(torch.diag(torch.sum(A0, dim = 1)))\n",
    "\n",
    "    \n",
    "# Optimization\n",
    "err = torch.zeros(iter)\n",
    "\n",
    "for t in range(iter):\n",
    "\n",
    "    for v in range(V):\n",
    "\n",
    "        # Updating Hs\n",
    "        Q = torch.sum(torch.stack(H)) + H[v]\n",
    "        \n",
    "        Hn = 2 * (W[v].T @ X[v])                    + 2 * gamma * (H[v] @ A[v])\n",
    "        Hd = 2 * (W[v].T @ W[v] @ H[v]) + alpha * Q + 2 * gamma * (H[v] @ D[v])\n",
    "        H[v] = H[v] * (Hn / torch.maximum(Hd, eps))\n",
    "        \n",
    "        # Updating Ws\n",
    "        Wn = X[v]        @ H[v].T \n",
    "        Wd = W[v] @ H[v] @ H[v].T \n",
    "        W[v] = W[v] * (Wn / torch.maximum(Wd, eps))\n",
    "        \n",
    "        \n",
    "\n",
    "        # Calculating cost function\n",
    "#         err[t] += torch.norm(X[v] - W[v] @ H[v]) ** 2 + gamma * torch.trace(H[v] @ (D[v] - A[v]) @ H[v].T) \n",
    "           \n",
    "# plt.plot(err)\n",
    "\n",
    "Hfinal = torch.mean(torch.stack(H), dim=0)\n",
    "\n",
    "pred = KMeans(n_clusters=c, n_init='auto').fit(Hfinal.T).labels_\n",
    "\n",
    "nmi, acc, ari, f1mi, f1ma, f1we, pur = eval(Y, pred)\n",
    "nmi, acc, ari, f1mi, f1ma, f1we, pur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
